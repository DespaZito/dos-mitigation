# Analysis Tools

## Overview

This folder contains tools for storing, parsing, and plotting experimental data generated by other tools in the repository.  Data is pre-parsed and stored in a PostgreSQL database, then pulled into pandas dataframes as needed for plotting, which is done using matplotlib and seaborn.

After following the setup procedure below, this process can be orchestrated using Jupyter notebooks:
- Use [import.ipynb](import.ipynb) to parse a session of data and add it to the database.
- Use [plot.ipynb](plot.ipynb) as a starting point for learning how to generate plots from the database.

If you'd like to take a more manual approach, those notebooks rely on helper functions defined in [db_parse.py](db_parse.py) and [db_helpers.py](db_helpers.py), to parse experiment outputs and to interface with the databse.

For very quick analysis and debugging purposes, you can use [csv_to_tps.py](csv_to_tps.py) to compute Transactions per Second from a single CSV file that has the format: `status, start_time, end_time`, which is what the provided [client scripts](../common/clients) will generate.

## Setup

*tested on Ubuntu 22.04*

1. Run `sudo db_setup.sh` to install dependencies, start a Postgres database service, and delete the postgres user's password to simplify interaction with the database (if you're concerned with data security you will probably want to modify this setup procedure).

2. Run `sudo -u postgres createuser --interactive` and enter your username when prompted.  This will allow you to interact with the databse directly instead of switching to the postgres user account.

3. `cd` into this analysis directory and run `python3 -m venv env` to create a python virtual environment in a new `env` directory (or simply `python -m venv env` if Python 3 is your system defualt).

4. Run `source env/bin/activate` to activate the new environment.

5. Run `pip3 install seaborn psycopg2 jupyter` to install necessary python libraries.

6. Move/copy your experiment data into the `data` directory (or specify an alternate path in the Jupyter notebooks).  If storing large amounts of data on an external volume, you can replace the data directory with a symlink to avoid copying/moving.  Each folder in the data directory should contain data from a single session.  The directory name will be used as the session nickname in the database.  Within the session directory should be a hidden copy of the session's parameters (`.parameters.json`), as well as some number of timestamped directories, one per experiment.  Within each of those should be one directory per host, and so on following the structure that [run.py](../run.py) creates.  If you're using all the automation tools.

7. Import data with `import.ipynb` and analyze it with `plot.ipynb`, making sure to change variables set at the top to match your actual session name, etc.

## Database Schema

The database schema is defined in [schema.sql](schema.sql) and illustrated in [schema.pdf](schema.pdf) (via [dbdiagram.io](https://dbdiagram.io)).  It is designed to bridge the Merge testbed workflow with the experimental methodology defined in my thesis, with reapeated sets of four experiments controlling for separate and combined effects of DoS attacks and mitigations across arbitrary variables.

The `revisions` and `materializations` tables contain information related to those aspects of the Merge testbed.  Currently this data must be input and updated manually (look for `revision_row` and `materialization_row` in [import.ipynb](import.ipynb)).  The assumption is that you'll typically run many sessions on the same topology (with a single revision and materialization).  These tables aren't currently used for much, but will be valuable if conducting experiments across different hardware.

Each `session` consists of multiple `experiments`, during which each `host` collects some `data`.  Key `results` are then extracted from the raw `data`.  To clarify, the `data` table essentially contains (metric, timestamp, value) tuples, while the `results` table aggregates those into higher-layer metrics like overhead and efficiency, across the full timespan of the experiment.  Raw data is preserved in case it's needed, but most plots should be easier and faster to generate from these higher-level results.

